// vim: set syntax=asciidoc:
[[create]]
== Creating Images
:data-uri:
:toc:
:toclevels 4:
:homepage https://github.com/projectatomic/container-best-practices:

The base unit of creating an image is the Dockerfile itself.  This section
focuses on the instructions that make up a Dockerfile.

This chapter will not cover every Dockerfile instruction available but instead
will focus on specific ones that we want to re-enforce to those who develop
Dockerfiles.  Docker has published a
link:https://docs.docker.com/engine/reference/builder/[reference guide] already
covering each of the Dockerfile instructions. In addition, upstream docker has a nice description of https://docs.docker.com/engine/articles/dockerfile_best-practices/[best practices] for Dockerfiles. It
describes the various instructions that can be used to compose a Dockerfile and their best usage. Familiarize yourself with these
recommendations.

=== Creating Base Images

==== Choosing Base Image

Images that have no parent are called base images. Docker images usually have their own root filesystem with an operating system installed. So when you want to create a new image, it either has to be based on an image that actually provides an operating system or you will need to create this layer in your image. The only difference to this are super minimal images that instead of an operating system provide only a single binary as described later in the text.
There is a wide variety of base images already available on Docker Hub, so the simplest solution is to use one from there. Here are a few things that should help you determine which base image will fit your needs:

* Linux distribution - Your personal preference and experience will play an important role in choosing one distribution over another. However, you should consider whether your containerized application requires specific libraries or tools from a specific system.

* Image size - Base images usually contain a minimal operating system with a set of tools needed for basic operations. To preserve your environment small and efficient, size should also be taken into account when picking the right base image. The size varies; you can take advantage of super small base images, such as 2MB busybox, or use a standard minimal operating system, such as Fedora or CentOS that are up to 200MB in size.

* Updates - Not all community images are necessarily rebuilt on a regular basis or when security vulnerabilities are addressed. You should therefore consider using base images from "official repositories" on Docker Hub, and confirm their update policy in advance.

==== Creating Base Image

Once you've considered all options and decided to create your own base image, the process will mostly depend on the distribution you chose. Note that the major distributions have their source files available on GitHub so you still might want to consider creating an issue or opening a pull request to suggest a change in the feature set or any adjustment.
https://docs.docker.com/engine/userguide/eng-image/baseimages/[Docker documentation] suggests two approaches to creating a base image: using tar and building an image "FROM scratch".

===== Using tar

Using the tar tool is a simple way how to build a base image. As a prerequisite, you will need to set up a directory structure for chroot with all items that you wish to be part of the base image. There are various tools that might help you with this, for example _debootstrap_ for Debian systems or _supermin_ for RPM-based systems.

Once you have your chroot directory ready, it is as simple as running:

```
# tar -C <chroot_dir> -c . | docker import - <new_image_name>
```

Note that docker provides a set of scripts for base image creation that take advantage of tar: https://github.com/docker/docker/tree/master/contrib[https://github.com/docker/docker/tree/master/contrib]. Popular distributions then use their own build systems that usually also utilizes tar. For example Fedora's https://fedoraproject.org/wiki/Koji/BuildingImages?rd=Koji/KojiLiveCDHowTo#Building_Disk_Images[koji].

===== FROM scratch

"scratch" is a special repository in the Docker Hub registry, created using an empty tarball. It is not meant to be pulled or run, and at any such an attempt you will most likely encounter this message: _'scratch' is a reserved name_.
Using scratch is ideal for creating extremely minimal images, for example for containerizing single binaries. An example is available from https://docs.docker.com/engine/userguide/eng-image/baseimages/[Docker documentation].
scratch is also very handy for creating standard distribution base images. But as with tar, you'll first need to prepare a directory structure for chroot. After that, just add the directory in your Dockerfile as follows:

```
FROM scratch
ADD <chroot_dir> /
CMD ["/bin/bash"]
```
// TBD: === Creating Layered Images

=== Creating System Images

The need for a container service to be started promptly before the Docker service starts supplies the requirements of a system container. The Open-vm-tools container is a system container utilizing runc as the runtime engine.

A system container normally starts as a regular Docker container:
[source,shell]
----
FROM rhel7:7.4-ondeck

LABEL summary="The open-vm-tools guest agent" \
      io.k8s.description="The open-vm-tools agent is providing information about the virtual machine and allows to restart / shutdown the machine via VMware products. This image is intended to be used with virtual machines running Red Hat Enterprise Linux Atomic Host." \
      name="rhel/open-vm-tools" \
      version="7.4" \
      com.redhat.component="open-vm-tools-docker" \
      maintainer="davis phillips <dphillip@redhat.com>"

ENV SYSTEMD_IGNORE_CHROOT=1

RUN yum-config-manager --enable rhel-7-server-rpms || :
RUN yum -y --setopt=tsflags=nodocs install file open-vm-tools perl net-tools iproute systemd
RUN yum clean all

COPY tmpfiles.template service.template config.json.template /exports/
COPY init.sh /usr/bin/

LABEL run="docker run  --privileged -v /proc/:/hostproc/ -v /sys/fs/cgroup:/sys/fs/cgroup  -v /var/log:/var/log -v /run/systemd:/run/systemd -v /sysroot:/sysroot -v=/var/lib/sss/pipes/:/var/lib/sss/pipes/:rw -v /etc/passwd:/etc/passwd -v /etc/shadow:/etc/shadow -v /tmp:/tmp:rw -v /etc/sysconfig:/etc/sysconfig:rw -v /etc/resolv.conf:/etc/resolv.conf:rw -v /etc/nsswitch.conf:/etc/nsswitch.conf:rw -v /etc/hosts:/etc/hosts:rw -v /etc/hostname:/etc/hostname:rw -v /etc/localtime:/etc/localtime:rw -v /etc/adjtime:/etc/adjtime --env container=docker --net=host  --pid=host IMAGE"

CMD /usr/bin/vmtoolsd
----

Note, the following line:

[source,shell]
----
COPY tmpfiles.template service.template config.json.template /exports/
----

This line sets up to stage the systems container. The important components for this are service.template and config.json.template. The service.template is the systemd unit template for when the systems container is installed via atomic install.

[source,shell]
----
[Unit]
Description=Service for virtual machines hosted on VMware
Documentation=http://github.com/vmware/open-vm-tools
ConditionVirtualization=vmware

[Service]
ExecStartPre=/bin/bash -c 'systemctl import-environment'
ExecStartPre=/bin/bash -c 'export -p > /tmp/open-vm-tools-bash-env'
ExecStart=$EXEC_START
ExecStop=$EXEC_STOP
WorkingDirectory=$DESTDIR

[Install]
WantedBy=multi-user.target
----

The config.json.template is similar to the Docker run label or line. Below shows the environment variables, bind mounts and the command to execute "/usr/bin/init.sh"

[source,shell]
----
{
    "ociVersion": "1.0.0",
    "platform": {
        "os": "linux",
        "arch": "amd64"
    },
    "process": {
        "terminal": false,
        "user": {},
        "args": [
            "/usr/bin/init.sh"
        ],
        "env": [
            "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
            "TERM=xterm",
            "NAME=open-vm-tools",
            "SYSTEMD_IGNORE_CHROOT=1"
        ],
...
omitted
    "mounts": [
            {
            "destination": "/run/systemd",
            "type": "bind",
            "source": "/run/systemd",
            "options": [
                "rw",
                "rbind",
                "rprivate"
            ]
        },
...
omitted
----

Often times, executing a single command via the container is not enough. The above command init.sh stages the container environment and ensures both VGAuthService and vmtoolsd is executed inside the container.

[source,shell]
----
#!/bin/sh
source /tmp/open-vm-tools-bash-env

COMMAND=/usr/local/bin/vmware-toolbox-cmd
if [ ! -e $COMMAND ]
  then
    echo 'runc exec -t open-vm-tools vmware-toolbox-cmd "$@"' > /usr/local/bin/vmware-toolbox-cmd
    chmod +x /usr/local/bin/vmware-toolbox-cmd
fi
exec /usr/bin/VGAuthService -s &
exec /usr/bin/vmtoolsd 
----

Here are the commands to execute via the atomic CLI to install and convert a system container. Provided we have already built the open-vm-tools container from the Dockerfile listed above. 

[source,shell]
----
atomic pull --storage=ostree docker:open-vm-tools
atomic install --system open-vm-tools
systemctl start open-vm-tools
----

Similarly, we can pull this container from the Red Hat registry and install it in the same fashion.
[source,shell]
----
atomic pull --storage ostree registry.access.redhat.com/rhel7/open-vm-tools
atomic install --system registry.access.redhat.com/rhel7/open-vm-tools
systemctl start open-vm-tools 
----

The atomic install command installs the systemd unit file from the container from its /exports/ directory and sets the service to enable. The systemctl command below that starts the service immediately instead of awaiting a reboot. 

More examples of system containers can be found https://github.com/projectatomic/atomic-system-containers[here]. This includes the open-vm-tools example for CentOS. 

// ==== Creating Component or Application Images

[[creating_concise]]
=== Small and Concise Images

It is preferable to create small and concise images whenever possible.  This can
be highly dependent on the application you are containerizing, but there are
techniques to help you accomplish this.  The following sections cover these
techniques.

==== Chaining Commands

In general, having fewer layers improves readability. Commands that are chained together become a part of the
same layer. To reduce the number of layers, chain commands together. Find a balance, though, between a large
number of layers (and a great many commands), and a small number of layers (and obscurity caused by brevity).

A new layer is created for every new instruction defined. This does not necessarily mean that one instruction
should be associated with only one command or definition.

Ensure transparency and provide a good overview of the content of each layer by grouping related operations
together so that they together constitute a single layer. Consider this snippet:

.Chained Dockerfile instruction
```
RUN dnf install -y --setopt=tsflags=nodocs \
    httpd vim && \
    systemctl enable httpd && \
    dnf clean all
```

Each command that is related to the installation and configuration of `httpd` is grouped together
as a part of the same layer. This meaningful grouping of operations keeps the number of layers low
while keeping the easy legibility of the layers high.

===== Using Double Ampersands (&&) vs Semi-colons (;)

In the RUN instruction of Dockerfiles, it is common to string together multiple commands for efficiency.  Stringing
commands together in the RUN instructions are typically done with ampersands or semi-colons. However, you should
consider the implications of each and their usage.  The following examples illustrate the difference.

.Using semi-colons as instruction conjunctions
```
RUN do_1; do_2
```

This sort of conjunction will be evaluated into do_1 and then do_2.  However, using the double
ampersands results in a different evaluation.

.Using double ampersands as conjunctions
```
RUN do_1 && do_2
```

The ampersands change the resulting evaluation into do_1 and then do_2 _only if do_1 was successful_.

The use of the double ampersands as conjunctions is a better practice in Dockerfiles because
it ensures that your instructions are completed or the build will fail.  If the build were to continue
and you had not closely monitored the build (or its results), then the image may not be exactly
as you desired.  This is particularly true with automated build systems where you will want any
failure to result in the failure of the build itself.

There are certainly use cases where semi-colons might be preferred and possibly should be used.
Nevertheless, the possible result of an incomplete image should be carefully considered.

==== Clearing Packaging Caches and Temporary Package Downloads

Package managers can typically generate lots of metadata and also store downloaded content into a cache of
sorts. To keep images and layers as small as possible, you should consider clearing out these caches of downloaded
content.  Note how the following example ends with _yum -y clean all_ which removes deletable yum content.

.A singular RUN instruction performing multiple commands
```
RUN yum install -y epel-release && \
    rpmkeys --import file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 && \
    yum install -y --setopt=tsflags=nodocs bind-utils gettext iproute\
    v8314 mongodb24-mongodb mongodb24 && \
    yum -y clean all
```

There are several package managers beyond yum that should be of note: dnf, rvm, gems, cpan, pip. Most of these
managers have some form of a clean-up command that will handle excess cache created while performing their package management duties.

Below are examples pictured for dnf and rvm:

.dnf cleanup example
```
RUN rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm && \
    dnf -y install nodejs tar sudo git-all memcached postgresql-devel postgresql-server \
    libxml2-devel libxslt-devel patch gcc-c++ openssl-devel gnupg curl which && \
    dnf clean all \
```

.Ruby (rvm) cleanup example
```
RUN /usr/bin/curl -sSL https://rvm.io/mpapis.asc | gpg2 --import - && \
    /usr/bin/curl -sSL https://get.rvm.io | rvm_tar_command=tar bash -s stable && \
    source /etc/profile.d/rvm.sh && \
    echo "gem: --no-ri --no-rdoc --no-document" > ~/.gemrc && \
    /bin/bash -l -c "rvm requirements && rvm install ruby 2.2.4 && rvm use 2.2.4 --default && \
    gem install bundler rake && \
    gem install nokogiri --use-system-libraries && \
    rvm cleanup all && yum clean all && rvm disk-usage all"
```

In the above example, notice the `yum clean all` command called after rvm; this is because some package managers like rvm rely on others (like yum
in this case) to help perform their duties. Make sure to examine your container's layers sizes to help determine where you can
eliminate excess size and keep its footprint size to a minimum.

Here is a listing of some package managers and the applicable cleanup commands:

.Package Managers
[cols="2*", options"header"]
|===
|Package Manager
|Cleanup Command

|yum
|yum clean all

|dnf
|dnf clean all

|rvm
|rvm cleanup all

|gem
|gem cleanup

|cpan
|rm -rf ~/.cpan/{build,sources}/*

|pip
|rm -rf ~/.cache/pip/*

|apt-get
|apt-get clean

|===

===== Clearing Package Cache and Squashing

If you squash your images after manual building or as part of an automated build process, it is not necessary to clean cache in every single relevant instruction/layer as the intermediate layers affect the previous ones in this case.

Simple example Dockerfiles below would both produce the same image if they were squashed:

.Cache cleanup in a separate instruction
```
FROM fedora
RUN dnf install -y mariadb
RUN dnf install -y wordpress
RUN dnf clean all
```

.Cache cleanup chained with the install command
```
FROM fedora
RUN dnf install -y mariadb wordpress && dnf clean all

```
However, without squashing, the first image would contain additional files and would be bigger than the second one:

.Size comparison
```
# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED              VIRTUAL SIZE
example             separate            54870d73715f        21 seconds ago       537.7 MB
example             chained             6a6156547888        About a minute ago   377.9 MB

```

Therefore, it is a good practice to write Dockerfiles in a way so that others can use it as a valid reference and are always able to reproduce the build. To ensure this, you should **clean cache in every layer** where applicable. In general, you should always aim to create images that are small and concise regardless of whether the final image is squashed or not.

Read more about suqashing and its repercussions in the link:#squashing[Squashing layers] section.

==== Removing Unnecessary Packages

In some cases, your image can end up with several packages that are not necessary to support the runtime of your
application.  A good example is when you actually build your application from source during the build of the image
itself.  Typically, when you build an application, you will pull in development (-devel) packages as well as
toolchain-based packages like make and gcc.  Once your application is built, you may no longer need these packages
for runtime depending on how your application links to libraries.

Depending on your application and which packages you added to your image, you might need to iteratively attempt to
remove packages checking to make sure your application still works.  One suggestion would be to remove big parts of the
toolchain.  And then use your package manager's command to clean up unused packages.  In the case of dnf, you can
remove unneeded packages like so:

.Removing unnecessary packages with dnf
```
# dnf autoremove
```

You should run this command in an interactive shell (docker run -it --rm <image> /bin/bash) initially so you can
get a feel for which packages will be removed.  One upside to doing so is that you can then test run your application
from the interactive shell to make sure it still works.

==== Removing Documentation
Another technique for reducing the image size is by limiting the documentation being installed. If you package manager supports such a thing and you have no expectations for users to use a shell to interact with your image, this might significantly reduce the size of your image.

Yum has an optional flag to not install documentation.  The following example shows how to set the flag.

----
RUN yum install -y mysql --setopt=tsflags=nodocs
----

Note that the `nodocs` flag is used in some base images, for example CentOS and Fedora, and this setting gets
inherited by the child layers. This can cause problems in case you decide to include the documentation in one of your layered images.

In this case, if you wish to have the documentation installed for **packages from your single layer only**, you have to
empty the **tsflags** option as follows:

----
RUN yum -y install docker --setopt=tsflags=''
----

If you wish to have the documentation installed for **packages from your single layer and the parent layers**, you need
 to reinstall the packages with the empty **tsflags** option as follow:

----
RUN yum -y reinstall "*" --setopt-tsflags='' && yum install docker --setopt-tsflags=''
----

In case you need to have documentation included for **every package from every single parent or child layer**,
the */etc/yum.conf* file needs to be edited as follows:

----
RUN [ -e /etc/yum.conf ] && sed -i '/tsflags=nodocs/d' /etc/yum.conf || true
RUN yum -y reinstall "*"
RUN yum -y install <package>
----

[[squashing]]
==== Squashing Layers

Each instruction you create in your Dockerfile results in a new image layer being created. Each layer brings additional
data that are not always part of the resulting image. For example, if you add a file in one layer, but remove it in
another layer later, the final image's size will include the added file size in a form of a special "whiteout" file
although you removed it. In addition, every layer contains separate metadata that add up to the overall image size as
well. So what are the benefits of squashing?

* **Performance** - Since all layers are copy-on-write file systems, it will take longer to build the final container
from many layers. Squashing helps reduce the build time.

* **Image size** - Similarly, since an image is actually a collection of other images, the final image size is the sum of the sizes of component images. With squashing, you can prevent these unwanted size additions.

* **Organization** - Squashing also helps you control the structure of an image, reduce the number of layers and organize images logically.

However, Docker does not yet support squashing natively, so you will have to work around it by using alternative
approaches, some of which are listed below.

===== Experimental --squash Flag
As of version 1.13, Docker supports the `--squash` flag that enabled the squashing functionality. This is currently only available in experimental mode.

===== docker save

You can use `docker save` to squash all the layers of your image into a single layer.  The _save_ command
was intended for this use, so this happens to be a side effect of the process. This approach,
however, is not very practical for sharing as the user will be able to only download
the whole content and cannot take advantage the caching. Note that the base image layer will be included
as well and might be several hundreds of megabytes in size.

[[squash_tools]]
===== Custom Tools

You will surely find a lot of utilities on the internet that facilitate layer squashing.
We recommend taking advantage of Marek Goldmann's https://github.com/goldmann/docker-squash[docker-squash], which
automates layer squashing, is kept up-to-date and has been tested by the community.


===== Repercussions of Squashing

* When you squash an image, you will lose the history together with the metadata accompanying the layers.
* Without the metadata, users building an image from a layered image that has been squashed are losing the idea that it happened.
* Similarly, if you decide to include the parent layer from which your image is built into the resulting squashed image, you ultimately prevent others from seeing that this happened.

Look at the mongodb example:

```
# docker images openshift/mongodb-24-centos7
REPOSITORY                               TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
docker.io/openshift/mongodb-24-centos7   latest              d7c0c18b0ae4        16 hours ago        593.3 MB
```

Without squashing, you can see the complete history and how each of the layers occupies space.

```
# docker history docker.io/openshift/mongodb-24-centos7:latest
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
d7c0c18b0ae4        About an hour ago   /bin/sh -c #(nop) CMD ["run-mongod"]            0 B
63e2ba112add        About an hour ago   /bin/sh -c #(nop) ENTRYPOINT &{["container-en   0 B
ca996db9c281        About an hour ago   /bin/sh -c #(nop) USER [184]                    0 B
8593b9473058        About an hour ago   /bin/sh -c #(nop) VOLUME [/var/lib/mongodb/da   0 B
5eca88b7872d        About an hour ago   /bin/sh -c touch /etc/mongod.conf && chown mo   0 B
9439db8f40ad        About an hour ago   /bin/sh -c #(nop) ADD dir:f38635e83f0e6943cd3   17.29 kB
12c60945cbac        About an hour ago   /bin/sh -c #(nop) ENV BASH_ENV=/usr/share/con   0 B
e6073f9a949f        About an hour ago   /bin/sh -c #(nop) ENV CONTAINER_SCRIPTS_PATH=   0 B
619bf2ae5ed8        About an hour ago   /bin/sh -c yum install -y centos-release-scl    342.6 MB
ab5deeccfe21        About an hour ago   /bin/sh -c #(nop) EXPOSE 27017/tcp              0 B
584ded9dcbca        About an hour ago   /bin/sh -c #(nop) LABEL io.k8s.description=Mo   0 B
17e3bcd28e07        About an hour ago   /bin/sh -c #(nop) ENV MONGODB_VERSION=2.6 HOM   0 B
807a1e9c5a7b        16 hours ago        /bin/sh -c #(nop) MAINTAINER SoftwareCollecti   0 B
28e524afdd05        10 days ago         /bin/sh -c #(nop) CMD ["/bin/bash"]             0 B
044c0f15c4d9        10 days ago         /bin/sh -c #(nop) LABEL name=CentOS Base Imag   0 B
2ebc6e0c744d        10 days ago         /bin/sh -c #(nop) ADD file:6dd89087d4d418ca0c   196.7 MB
fa5be2806d4c        7 months ago        /bin/sh -c #(nop) MAINTAINER The CentOS Proje   0 B

```
See how the history and size changes after squashing all layers in a single one (using the script link:#squash_tools[above]):

```
# docker history docker.io/openshift/mongodb-24-centos7:squashed
IMAGE               CREATED             CREATED BY          SIZE                COMMENT
90036ed9bd1d        58 minutes ago                          522.1 MB

```

* One of the biggest benefits of using layers is the posibility to reuse them. Images are usually squashed into a single big layer, which does not allow for pushing partial updates in individual layers; instead, the whole image needs to be pushed into the registry upon a change. The same applies to pulling the image from the registry.
* Some users might rely on suqashing when it comes to sensitive data. Be cautios because squashing is not meant to "hide" content. Even though squashing removes intermediate layers from the final image, information about secrets used in those layers will stay in the build cache.

=== Labels

Labels in Dockerfiles serve as a useful way to organize and document metadata used to describe an image.  Some labels are only descriptive
by nature, like  _name_ whereas others, like _RUN_ can be used to describe action-oriented metadata.  Labels are often leveraged by applications, like
https://github.com/projectatomic/atomic[atomic] or https://github.com/openshift[OpenShift], to help the image run as the author intended. Labels are primarily intended for descriptive purposes and can be viewed manually with the `docker inspect <image_name>` command.

==== When are Labels Required?

Labels are never required per se unless your build system or lifecycle management process requires them.
However, the use of labels is highly recommended for a number of reasons:

* As mentioned above, many container related tools can use the label metadata in meaningful ways often
contributing to a better user experience.
* The label metadata is always visible when inspecting the image.  Therein, users can at least see the
metadata even if their tooling does not make specific use of it.  For example, the RUN label basically
documents how you, as the author of the Dockerfile, expect this image to be run.

==== Descriptive Labels

The descriptive labels usually are alpha-numeric strings used to describe some aspect of the image itself.  Examples, might be
the version and release labels which could theoretically just be integer based.
The following table describes labels that are meant to be purely descriptive in nature.

.Descriptive labels
[options="header,footer"]
|===============================================
| Label | Description | Example
| name 	| Name of the Image | _"rhel7/rsyslog"_
| version | Version of the image | _"7.2"_
| release | Release number of the image | _"12"_
| architecture | Architecture for the image | _"x86_64"_
| build-date | Date/Time image was built as https://tools.ietf.org/html/rfc3339[RFC 3339] date-time | _"2015-12-03T10:00:44.038585Z"_
| vendor | Owner of the image | _"Red Hat, Inc."_
| URL | URL with more information about the image | _TBD_
| Summary | Brief description of the image | _TBD_
| Description | Longer description of the image | _TBD_
| vcs-type | The type of version control used by the container source. Generally one of git, hg, svn, bzr, cvs | _"git"_
| vcs-url |URL of the version control repository | _TBD_
| vcs-ref | A 'reference' within the version control repository; e.g. a git commit, or a subversion branch | _"364a...92a"_
| authoritative-source-url |	The authoritative location in which the image is published | _TBD_
| distribution-scope |Intended scope of distribution for image. Possible values are private, authoritive-source-only, restricted, or public  | _private_
| changelog-url | URL of a page containing release notes for the image| _TBD_
|===============================================

==== Action-Oriented Labels
[[label_action]]
Most action-oriented labels will be a used in the context of a docker command in order for the container to behave in a desired
way.  The following table describes the defined action-oriented labels.

.Action-oriented labels
[options="header,footer"]
|===============================================
| Label | Description | Example
| help | Command to run the help command of the image | _tbd_
| run | Command to run the image | _"docker run -d --privileged --name NAME --net=host --pid=host -v /etc/pki/rsyslog:/etc/pki/rsyslog -v /etc/rsyslog.conf:/etc/rsyslog.conf -v /etc/sysconfig/rsyslog:/etc/sysconfig/rsyslog -v /etc/rsyslog.d:/etc/rsyslog.d -v /var/log:/var/log -v /var/lib/rsyslog:/var/lib/rsyslog -v /run:/run -v /etc/machine-id:/etc/machine-id -v /etc/localtime:/etc/localtime -e IMAGE=IMAGE -e NAME=NAME --restart=always IMAGE /bin/rsyslog.sh"_
| uninstall | Command to uninstall the image | _"docker run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=IMAGE -e NAME=NAME IMAGE /bin/uninstall.sh"_
| install |	Command to install the image | _"docker run --rm --privileged -v /:/host -e HOST=/host -e IMAGE=IMAGE -e NAME=NAME IMAGE /bin/install.sh"_
| stop | Command to execute before stopping container | _tbd_
| debug | Command to run the image with debugging turned on | _tbd_
|===============================================

==== Recommended Labels for your Project

Labels are critical to properly identifying your image and influencing how it runs.  For the purposes of
identification, we recommend that you at least use the following labels:

* name
* version
* release
* architecture
* vendor

And for actionable labels, we recommend you use at least the following:

* RUN
* INSTALL
* UNINSTALL

These three are the most critical for ensuring that users run the image in the manner you wish.  Furthermore,
tools developed to read and act upon this meta data will work correctly.

In the case that you provide a help file that does not follow the standard of a man page, then the HELP label would also
be prudent.

==== Recommended Labels for your OpenShift Project

Images that are meant to be run in OpenShift are recommended to contain a set of labels as seen in the https://docs.openshift.org/latest/creating_images/metadata.html[OpenShift Origin documentation]. The labels are namespaced in compliance with the https://docs.docker.com/engine/userguide/labels-custom-metadata/[Docker format]; that is _io.openshift_ for OpenShift and _io.k8s_ for Kubernetes.

See the following example snippet from the https://github.com/openshift/s2i-ruby/blob/master/2.2/Dockerfile[s2i-ruby] image:

```
LABEL io.k8s.description="Platform for building and running Ruby 2.2 applications" \
      io.k8s.display-name="Ruby 2.2" \
      io.openshift.expose-services="8080:http" \
      io.openshift.tags="builder,ruby,ruby22"

```

For a comprenehsive list of recommended labels you might want to consider for your projects, see the https://github.com/projectatomic/ContainerApplicationGenericLabels[Container Application Generic Labels] git repository.

=== Template
// Link to https://github.com/container-images/container-image-template and https://github.com/devexp-db/distgen/


=== Starting your Application

Generally the CMD instruction in the Dockerfile is used by docker to start your application
when the image or container is started.  In the planning section, we provided some reasoning
for choosing how to  *_xref:planning_starting_application[start your application]_*.  The following
subsections will show how to implement each choice in your Dockerfile.

==== Calling the Binary Directly
Being the simplest of the choices, you simply need to call the binary using the CMD instruction or
define an ENTRYPOINT in your Dockerfile.

```
CMD ["/usr/bin/some_binary"]
```

===== Using the CMD Instruction
With CMD, you can identify the default command to run from the image, along with options you want to pass to it.
If there is no ENTRYPOINT in the Dockerfile, the value of CMD is the command run by default when you start the
container image. If there is an ENTRYPOINT in the Dockerfile, the ENTRYPOINT value is run as the command instead,
with the value of CMD used as options to the ENTRYPOINT command.

The CMD instruction can be overridden when you run the image. Any time you add an argument to the end of a docker run command, the CMD instruction inside the container is ignored. For example, when running `docker run -it myimage bash`, whatever command is set in the CMD instruction in your Dockerfile will be overrriden and _bash_ will be run instead.

===== Using the ENTRYPOINT Instruction
Like CMD, the ENTRYPOINT instruction lets you define the command executed when you run the container image, but it
cannot be overridden by arguments you put at the end of the `docker run` command. If your Dockerfile includes an
ENTRYPOINT instruction and there is also a CMD instruction, any arguments on the CMD instruction line are passed to
the command defined in the ENTRYPOINT line.

This is the distinct advantage of the ENTRYPOINT instruction over the CMD instruction because the command being run
is not overridden but it can be subsidized.  Suppose you have an ENTRYPOINT instruction that displays two files.
You could easily add an additional file to be displayed by adding it to the docker run command.

You can override the ENTRYPOINT command by defining a new entrypoint with the `--entrypoint=""` option on the docker
command line.

[[creating_using_a_script]]

==== Using a Script
Using a script to start an application is very similar to calling the binary directly. Again, you
use the CMD instruction but instead of pointing at the binary you point at your script that was
injected into the image.  The _registry.access.redhat.com/rhel7/rsyslog_ image uses a script
to start the rsyslogd application. Lets look at the two relevant instructions in its Dockerfile
that make this happen.

The following instruction injects our script (rsyslog.sh) into the image in the _bin_ dir.
```
ADD rsyslog.sh /bin/rsyslog.sh
```

The contents of the script are as follows:

```
#!/bin/sh
# Wrapper to start rsyslog.d with appropriate sysconfig options

echo $$ > /var/run/syslogd.pid

source /etc/sysconfig/rsyslog
exec /usr/sbin/rsyslogd -n $SYSLOGD_OPTIONS
```

Notice how the script does in fact handle environment variables by sourcing the _/etc/sysconfig/rsyslog_
file. And the CMD instruction simply calls the script.

```
CMD [ "/bin/rsyslog.sh" ]
```

==== Displaying Usage Information

It might not always be desired to start an application right away. In such a case, we can display usage information with a script instead.
A good example are builder images that are used, as the name suggests, for building applications rather than being run standalone.
Let's take link:https://github.com/openshift/s2i-python[s2i-python] as an example:

```
$ docker run docker.io/centos/python-35-centos7
This is a S2I python-3.5 centos base image:
To use it, install S2I: https://github.com/openshift/source-to-image

Sample invocation:

s2i build https://github.com/sclorg/s2i-python-container.git --context-dir=3.5/test/setup-test-app/ centos/python-35-centos7 python-sample-app


You can then run the resulting image via:
docker run -p 8080:8080 python-sample-app
```

The s2i-python image leverages the link:https://github.com/openshift/source-to-image[source-to-image] tool to build Python applications from the user's source code. Because the image usage is rather specific and requires user's input, providing the instructions by default is very convenient.

So, just like the example in the previous section uses a script as _CMD_ input to run the application, the script in this example outputs valuable information about the image usage. 

If you would like to provide more information about your image or don't want to pass the usage information as the default command, consider including a link:#creating_help_file[help file] instead.

[[creating_using_systemd]]
==== Using systemd Inside the Container

Extending our example from link:#creating_using_a_script[starting an application with a script], the rsyslog
image was started with a script.  We could easily use systemd to start the application.  To use systemd
to start a service that has a unit file, we need to tell systemd to enable the service and then let the
init process handle the rest. So instead of the ADD instruction used earlier, we would use a RUN
instruction to enable the service.

```
RUN systemctl enable rsyslog
```

And then we need to change the CMD instruction to call _/usr/sbin/init_ to let systemd take over.

```
RUN /usr/sbin/init
```

==== Using Systemd to Control Containers

The control mechanism for most docker functions is done via the docker commands or something like
the atomic application which simplifies the management of containers and images for users.  But in
a non-development environment, you may wish to treat your containers more like traditional services
or applications.  For example, you may wish to have your containers start in a specific order on
boot-up.  Or perhaps you wish to be able to restart (or recycle) a container because you have changed
its configuration file.

There are several approaches to these sorts of function.  You can make sure a specific container
always starts on boot-up using the `--restart` switch with the docker command line when you initially
run the image.  There are also orchestration platforms like Kubernetes that will allow you to determine the
start up order of containers even when they are distributed.  But in the case where all the containers
reside on a single node, systemd might just be exactly the solution.  Like with traditional services,
systemd is capable of making sure services both start and in the order they are specified.  Moreover,
any issues with startup or the container are logged like any other system service.

When using systemd to manage your containers, you are really using systemd to call docker commands (and
subsequently the docker daemon) to perform the actions.  Therefore, once you commit to using systemd
to control a container, you will need to make sure that all start, stop, and restart actions are
conducted with systemd.  Failure to do so essentially decouples the docker daemon and systemd causing
systemd to be out of sync.

In review, systemd is a good solution for:

* host system services such as agents and long-running services
* logging via journald
* service dependant management
* traditional service management vis _systemctl_
* multi-container applications with dependencies on the same node


The configuration file below is a sample service file that can be used and edited to control your image or
container.  In the [Unit] section, you can declare other services needed by your image including the cases where
those services are also images.

.Sample template for a systemd service file
```
[Unit]
After=docker.service
Requires=docker.service
PartOf=docker.service
After=[cite another service]
Wants=[cite another service]

[Service]
EnvironmentFile=[path to configuration file]
ExecStartPre=-[command to execute prior to starting]
ExecStart=[command to execute for start]
ExecStartPost=/usr/bin/sleep 10
ExecStop=[command to execute for stop]
Restart=always

[Install]
WantedBy=docker.service
```

In the [Service] section, you can also declare the actual commands that should be run prior to start, in the
case of start, and in the case of stop.  These commands can either be straight base commands or docker run (or stop)
commands as well.  Finally, if you are using a well made image that contains labels like STOP or RUN, you
could also use the atomic command.  For example, a start command could simply be:

```
atomic run <image_name>
```

This works because the actual docker command to run that image is part of the image's metadata and atomic is
capable of extracting it.

The [Service] section also has an option for EnvironmentFile.  In a traditional, non-containerized systemd service,
this configuration file resides in _/etc/sysconfig/<service_name>_.  In the case of a containerized application,
these configuration files are not always configurable and therefore do not reside on the host's filesystem.  And
in the case of where they are configurable, the EnvironmentFile is usually more important to how the service
application is started.  If you are link:#planning_use_systemd[starting the application] within an image
with systemd, then systemd will use _/etc/sysconfig/<service_name>_ within the image itself.

For more information on writing unit files see link:https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/sect-Managing_Services_with_systemd-Unit_Files.html[Managing Systemd unit files].

// Help section

[[creating_help_file]]
=== Creating a Help File
include::help.adoc[]

=== The Dockerfile Linter

==== What Is the Linter?

The Dockerfile-lint is a rule based link:https://en.wikipedia.org/wiki/Lint_(software)['linter'] for verifying Dockerfiles. The rules are used to check file syntax and best practice options for things such as:

* Was yum clean up evoked after a package installation?
* In the RUN section did the writer link commands via semicolons or double ampersands?

These are determined by the rules author and are typically defined by best practices and writer requirements. The input rules are defined via a set of link:http://www.yaml.org/[yaml files].

At the time of this writing, there are a number of templates from base to automated build configurations.

==== Where do I Get the Linter and How do I Install it?

There are two iterations of the linter.

* The CLI version of the project is available here:
link:https://github.com/projectatomic/dockerfile_lint[Dockerfile_lint]
* The online version of the project is available here:
link:https://access.redhat.com/labs/linterfordockerfile/#/[Online linter]

For the next section, we will assume that the CLI version of the linter will be installed manually via link:https://docs.npmjs.com/[npm] using the following commands:

```
git clone https://github.com/projectatomic/dockerfile_lint/
cd dockerfile_lint
npm install
```
==== Where do I Get the Templates?

===== Built-in Templates
In the config directory, there are two base ruleset files. If the dockerfile_lint is executed without -r these are the base rules used.

* dockerfile_lint/config/default_rules.yaml

* dockerfile_lint/config/base_rules.yaml

===== Additional Types of Templates

In the sample_rules directory there are some included templates for OpenShift and an example base template.

* Basic rules - dockerfile_lint/sample_rules/basic_rules.yaml

This set of rules is a basic catchall of your typical Dockerfile. Things such as yum cache clean up and command execution etiquette are checked. These are the rules we will be referencing below.

* OpenShift Template - dockerfile_lint/sample_rules/openshift.yaml

In addition to testing the semantics of the basic template from above, The OpenShift template checks for some link:https://docs.openshift.org/latest/creating_images/metadata.html#defining-image-metadata[required OpenShift labels] specific to its use.

==== How do I Read and Customize the Templates?

The filename of the basic template is included in the command above: sample_rules/basic_rules.yaml

The rules are implemented using regular expressions matched on instruction of the Dockerfile. The rule file has 3 sections: a *profile* section, a *line rule* section and a *required instructions* section.

===== Profile Section

The profile section gives information about the rule file. This is the name identifier and description for the profile. This information should help users to identify an applicable template.
```
profile:
  name: "Default"
  description: "Default Profile. Checks basic syntax."
  includes:
    - recommended_label_rules.yaml
```
An excerpt from the rules shows how includes are defined:

```
includes:
  - recommended_label_rules.yaml
```
The include section allows for chaining rulesets of multiple sources. In the above example the recommended_label_rules.yaml is processed in addition to its source.

===== Line Rule Section

This section contains rules match on a given instruction in the Dockerfile. The line rules do the bulk of the dockerfile parsing.

The example below shows rules to run against the 'FROM' instruction.
```
  FROM registry.access.redhat.com/rhel7:latest
```

The excerpt below checks for the latest flag in the 'FROM' line.

```
line_rules:
  FROM:
    paramSyntaxRegex: /^[a-z0-9./-]+(:[a-z0-9.]+)?$/
      rules:
        -
          label: "is_latest_tag"
          regex: /latest/
          level: "error"
          message: "base image uses 'latest' tag"
          description: "using the 'latest' tag may cause unpredictable builds. It is recommended that a specific tag is used in the FROM line or *-released which is the latest supported release."
          reference_url:
            - "https://docs.docker.com/reference/builder/"
            - "#from"
```
Here is another example that parses the 'RUN' line.
```
  RUN yum -y --disablerepo=\* --enablerepo=rhel-7-server-rpms install yum-utils && \
    yum-config-manager --disable \* && \
    yum-config-manager --enable rhel-7-server-rpms && \
    yum clean all

    RUN yum -y install file open-vm-tools perl open-vm-tools-deploypkg net-tools && \
    yum clean all
```

The regex below checks to see if the yum command has been issued. If it has, check to see if _yum clean all_ has been run as well.
```
  RUN:
    paramSyntaxRegex: /.+/
      rules:
        -
           label: "no_yum_clean_all"  #This is a short description of the rule
           regex: /yum(?!.+clean all|.+\.repo)/g  #regex the linter is attempting to match
           level: "warn" # warn, error or info: These results will define how the linter exits
           message: "yum clean all is not used"
           description: "the yum cache will remain in this layer making the layer unnecessarily large"
           reference_url:
             - "http://docs.projectatomic.io/container-best-practices/#"
             - "_clear_packaging_caches_and_temporary_package_downloads"
            # Lastly, any best practice documentation that may be pertinent to the rule
```
===== Required Instructions Section

While the line rules section uses regex, the required instructions looks for the instantiation of the instruction.

```
required_instructions:
  -
    instruction: "EXPOSE"
    count: 1
    level: "info"
    message: "There is no 'EXPOSE' instruction"
    description: "Without exposed ports how will the service of the container be accessed?"
    reference_url:
      - "https://docs.docker.com/reference/builder/"
      - "#expose"
```

==== How do I use the Linter?

Execution of the CLI version of the linter may look like this:

```
dockerfile_lint -f /path/to/dockerfile -r sample_rules/basic_rules.yaml
```

Here is some sample output from the command above:

```
--------ERRORS---------

ERROR: Maintainer is not defined. The MAINTAINER line is useful for identifying the author in the form of MAINTAINER Joe Smith <joe.smith@example.com>.
Reference -> https://docs.docker.com/reference/builder/#maintainer

--------INFO---------

INFO: There is no 'ENTRYPOINT' instruction. None.
Reference -> https://docs.docker.com/reference/builder/#entrypoint

```
By default, the linter runs in strict mode (errors and/or warnings result in non-zero return code). Run the command with '-p' or '--permissive to run in permissive mode:
```
dockerfile_lint  -p -f /path/to/dockerfile
```
This allows for quick and automated testing as what is informational and what needs to be addressed immediately.
